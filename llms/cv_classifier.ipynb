{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552f7439-9171-4eb7-9168-1278b61f38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCLUSIONS\n",
    "#\n",
    "# the key point is to retrieve the senioprity from the CV (overall and specific)\n",
    "# we could use another tool to extract it and send it to the LLM already gathered to improve its performance\n",
    "# alternative: use the LLM to get every experience the candidate has and calculate and store the experience using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78532789-97ed-48db-91ce-2cd4ad13b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import pprint\n",
    "import os\n",
    "\n",
    "seniority = \"senior\"\n",
    "\n",
    "job_desc_text = f\"\"\"\n",
    "Position Description:\n",
    "\n",
    "We are seeking a {seniority} Data Engineer to design, build, and maintain scalable data pipelines. The ideal candidate will have expertise in SQL, ETL processes, and cloud technologies, collaborating closely with Data Scientists to ensure data integration and quality.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Manage cloud data storage systems.\n",
    "Collaborate with data scientists to meet data requirements.\n",
    "Ensure data quality and security.\n",
    "Automate data processes.\n",
    "Monitor and troubleshoot data systems.\n",
    "Optimize Big Data solutions.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Education: Degree in Computer Science or related field.\n",
    "Proficiency in programming languages such as Python, Go, or Rust.\n",
    "Strong SQL skills.\n",
    "Experience with Hadoop and Kafka.\n",
    "Familiarity with cloud platforms (IBM Cloud, Oracle Cloud).\n",
    "Knowledge of data orchestration tools like Prefect or Luigi.\n",
    "Experience in CI/CD tools (GitLab CI, CircleCI).\n",
    "\n",
    "Desirable:\n",
    "\n",
    "Experience with Snowflake.\n",
    "Knowledge of visualization tools (Tableau, PowerBI).\n",
    "Familiarity with Docker or Kubernetes.\n",
    "Understanding of agile methodologies.\n",
    "Cloud or big data certifications.\n",
    "Multicultural experience.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f23468-20ea-407e-ac85-cc8e2cf6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Función para leer el contenido de un archivo .docx\n",
    "def read_docx(cv_path):\n",
    "    try:\n",
    "        # Cargar el documento\n",
    "        doc = Document(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = []\n",
    "        for parrafo in doc.paragraphs:\n",
    "            contenido.append(parrafo.text)\n",
    "        \n",
    "        return '\\n'.join(contenido)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_doc(cv_path):\n",
    "    try:\n",
    "        # Inicializar la aplicación de Word\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        \n",
    "        # Abrir el documento\n",
    "        doc = word.Documents.Open(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = doc.Content.Text\n",
    "        \n",
    "        # Cerrar el documento y la aplicación de Word\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        \n",
    "        return contenido\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_cv(cv_path):\n",
    "    cv_text = read_pdf(cv_path)\n",
    "    if cv_text is None:\n",
    "        cv_text = read_docx(cv_path)\n",
    "        if cv_text is None:\n",
    "            cv_text =read_doc(cv_path)\n",
    "\n",
    "    return cv_text\n",
    "\n",
    "\n",
    "def get_job_description(url):\n",
    "    class_name = 'wiki-content'\n",
    "    # Hacer la solicitud HTTP\n",
    "    response = requests.get(url, verify = False)\n",
    "    \n",
    "    # Verificar que la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Analizar el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encontrar todos los elementos con la clase especificada\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "        \n",
    "        # Extraer el texto de esos elementos\n",
    "        text = '\\n'.join([element.get_text(separator='\\n').strip() for element in elements])\n",
    "        return text\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch the page. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3430549e-7dd9-4de9-8d28-ea5163613c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alan susa.pdf\n",
      "\n",
      "ALAN SUSA\n",
      "Data Engineer\n",
      "alansusa@email.com (123) 456-7890 New York, NY\n",
      "LinkedIn\n",
      "WORK EXPERIENCE EDUCATION\n",
      "Data Engineer B.A.\n",
      "Computer Science\n",
      "Consumer Reports\n",
      "University of Pittsburgh\n",
      "May 2018 - current New York, NY\n",
      "Led the migration from Oracle to Redshift using Amazon Athena September 2010 - April 2014\n",
      "and S3, resulting in an annual cost savings of $678,000 and an Pittsburgh, PA\n",
      "increase in performance of 14%\n",
      "Designed and implemented a real-time data pipeline to process\n",
      "semi-structured data by integrating 150 million raw records SKILLS\n",
      "from 30+ data sources using Kafka and PySpark\n",
      "Designed the data pipeline architecture for a new product that Python\n",
      "quickly scaled from 0 to 125,000 daily active users\n",
      "ETLs\n",
      "Studied and revamped data dictionaries to include a more\n",
      "SQL (Postgres, Redshift, MySQL)\n",
      "robust history for developing consistency across domain\n",
      "NoSQL (MongoDB)\n",
      "Spark, Kafka\n",
      "Data Engineer\n",
      "Airflow\n",
      "Guardian Life Insurance Company AWS (Athena, Lambda, S3)\n",
      "August 2016 - May 2018 New York, NY\n",
      "Maintained data pipeline up-time of 99.8% while ingesting\n",
      "streaming and transactional data across 8 different primary\n",
      "data sources using Spark, Redshift, S3, and Python\n",
      "Automated ETL processes across billions of rows of data, which\n",
      "reduced manual workload by 29% monthly\n",
      "Ingested data from disparate data sources using a combination\n",
      "of SQL, Google Analytics API, and Salesforce API using Python to\n",
      "create data views to be used in BI tools like Tableau\n",
      "Communicated with project managers and analysts about data\n",
      "pipelines that drove efficiency KPIs up by 26%\n",
      "Data Engineer Intern\n",
      "Federal Reserve Board of Governors\n",
      "August 2014 - August 2016 Washington, DC\n",
      "Built basic ETL that ingested transactional and event data from\n",
      "a web app with 12,000 daily active users that saved over\n",
      "$85,000 annually in external vendor costs\n",
      "Worked with client to understand business needs and translate\n",
      "those business needs into actionable reports in Tableau, saving\n",
      "17 hours of manual work each week\n",
      "Used Spark in Python to distribute data processing on large\n",
      "streaming datasets, improving ingestion and speed by 67%\n",
      "Supported implementation and active monitoring of controls\n",
      "and programs for precision and efficacy\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Brandon Connor.pdf\n",
      "\n",
      "BRANDON CAREER OBJECTIVE\n",
      "CONNOR Motivated by data and results, my passion and intern experience\n",
      "combined will exceed expectations at a company like Simplex. Ready\n",
      "to join a team of critical thinkers to build a data stack from scratch for\n",
      "Data Engineer\n",
      "the e-commerce space.\n",
      "brandonconnor@email.com\n",
      "(123) 456-7890 WORK EXPERIENCE\n",
      "Austin, TX\n",
      "Data Engineering Intern\n",
      "LinkedIn\n",
      "Balyasny\n",
      "July 2020 - current Austin, TX\n",
      "· Used Python, SQL, and Spark to collaborate with 2 interns and a\n",
      "EDUCATION\n",
      "junior data engineer to create a cloud-first data ingestion that\n",
      "B.S. improved processing speed of data by 74%\n",
      "Computer Science · Partnered with interns to construct a plug-in that improved\n",
      "investors' experience on the platform by 32%\n",
      "University of Texas\n",
      "· Ingested data from disparate sources using SQL and Google\n",
      "August 2016 - May 2020\n",
      "Analytics API to construct data views for BI tools like Tableau\n",
      "Austin, TX\n",
      "· Communicated with investors to understand needs, and\n",
      "GPA: 3.75 translated their feedback into actionable reports in Tableau,\n",
      "saving 46 hours of manual work each month\n",
      "RELEVANT COURSES\n",
      "Math Tutor\n",
      "Data Structures Breakthrough Central Texas\n",
      "Algorithm Design\n",
      "August 2017 - June 2020 Austin, TX\n",
      "Database Management Systems · Prepared and executed math curriculum for 3-4 students 5\n",
      "Computer Vision hours per week in small group sessions\n",
      "Software Design Methodology · Created engaging assessments, including learning games, bell\n",
      "ringers, and team quizzes to boost math scores by 12%\n",
      "· Communicated with parents 2 times per month, reporting\n",
      "SKILLS student progress while working together to adjust goals\n",
      "· Fostered relationships with students to build trust, staying after\n",
      "Python\n",
      "sessions to play basketball or football\n",
      "SQL\n",
      "ETLs\n",
      "APIs PROJECTS\n",
      "Spark\n",
      "University of Texas Hackathon\n",
      "AWS (Redshift)\n",
      "· Led the data ingestion efforts for our 3-person team, developing\n",
      "a real-time tracker of campus events for universities in Texas\n",
      "· Built web scraper in Python to acquire data from campus\n",
      "groups' websites, and built an ETL\n",
      "· Won 1st place overall out of 22 competing campus teams\n",
      "· Scaled web app to over 30 universities in Texas, resulting in\n",
      "1,700 monthly active users across those campuses\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Elara Quinn.pdf\n",
      "\n",
      "Elara Quinn\n",
      "Data Engineer Intern\n",
      "e.quinn@email.com (123) 456-7890 Austin, TX LinkedIn\n",
      "WORK EXPERIENCE\n",
      "HP Inc. - Data Engineer Intern\n",
      "2024 - current Austin, TX\n",
      "Built pipelines with Apache Hadoop, processing 4 terabytes of raw data for analytics projects\n",
      "Created Tableau dashboards visualizing metrics from over 52,088 customer transactions\n",
      "Leveled up data ingestion from 14 sources via Apache NiFi, ramping up pipeline efficiency\n",
      "Capitalized on MySQL to create and manage relational databases, improving data retrieval times for\n",
      "datasets exceeding 2.1M records\n",
      "PROJECTS\n",
      "Protocol Development - Volunteer\n",
      "Analyzed Amazon Redshift’s GDPR compliance role, proposing architecture for 428 datasets\n",
      "Assisted in designing a Tableau report tracking and document data access patterns\n",
      "Engineered a MySQL database to store sensitive client data, ensuring authorized access only\n",
      "Automated data breach alert notifications using AWS Lambda, expediting response by 9%\n",
      "Governance Research - Class Research Project Owner\n",
      "Presented a use case for Apache NiFi's role in automating data governance tasks\n",
      "Supported class discussions with visual aids, building two Tableau dashboards demonstrating data\n",
      "stewardship metrics\n",
      "Ramped up cloud data governance through detailed AWS IAM and KMS analysis\n",
      "Recommended client data security measures, including encryption and access controls\n",
      "EDUCATION\n",
      "University of Texas - Bachelor of Science, Computer Science\n",
      "2021 - current Austin, TX\n",
      "SKILLS\n",
      "MySQL; Apache NiFi; Amazon Redshift; Apache Hadoop; AWS; Tableau\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Ianthe Marlow.pdf\n",
      "\n",
      "IANTHE WORK EXPERIENCE\n",
      "MARLOWE Data Engineer\n",
      "Facebook\n",
      "Data Engineer\n",
      "2023 - current Austin, TX\n",
      "Instituted and maintained robust data pipelines in Apache\n",
      "i.marlowe@email.com Airflow, automating workflow for 3M user data points and\n",
      "reducing manual intervention by six hours per month\n",
      "(123) 456-7890\n",
      "Improved query performance and efficiency by integrating\n",
      "Austin, TX\n",
      "Presto into data retrieval systems\n",
      "LinkedIn\n",
      "Optimized data storage and management for structured\n",
      "datasets in Apache Hive, reducing data processing time for\n",
      "large datasets by 22%\n",
      "EDUCATION Built data warehousing solutions using Amazon S3, enabling\n",
      "seamless integration with other cloud services\n",
      "Bachelor of Science\n",
      "Computer Science\n",
      "Software Engineer\n",
      "Carnegie Mellon\n",
      "DVI Technologies\n",
      "University\n",
      "2020 - 2023 Pittsburgh, PA\n",
      "2015 - 2019\n",
      "Adopted PyTorch to develop machine learning models for\n",
      "Pittsburgh, PA\n",
      "image recognition, boosting classification accuracy by 18% in\n",
      "real-world apps\n",
      "Spearheaded the real-time event streaming architecture\n",
      "SKILLS\n",
      "using Apache Kafka, enabling high-speed data ingestion at\n",
      "rates of 15,036 messages per second\n",
      "Presto\n",
      "Refactored backend services by integrating Hack language,\n",
      "Apache Hive\n",
      "boosting server-side performance and lowering processing\n",
      "Apache Airflow\n",
      "times for web requests by two hours\n",
      "Amazon S3\n",
      "Streamlined development collaboration with Phabricator,\n",
      "PyTorch accelerating code reviews by 58%\n",
      "Apache Kafka\n",
      "MySQL\n",
      "Software Development Intern\n",
      "Hack\n",
      "Pittsburgh Steelers\n",
      "Phabricator\n",
      "2019 - 2020 Pittsburgh, PA\n",
      "Tableau\n",
      "Constructed complex SQL queries in MySQL to enhance the\n",
      "querying capabilities of performance data for players,\n",
      "reducing report generation time by 17%\n",
      "Facilitated the creation of interactive Tableau dashboards,\n",
      "enabling coaches to track 54 player performance metrics\n",
      "during the season\n",
      "Worked with the development team to troubleshoot and\n",
      "resolve software bugs, accelerating the delivery of products\n",
      "Participated in code reviews and debugging sessions,\n",
      "reviewing over 32 code contributions and improving the\n",
      "overall codebase by identifying and resolving critical issues\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "john smith.pdf\n",
      "\n",
      "JOHN SMITH\n",
      "DATA ENGINEER LOS ANGELES, CA 90291, UNITED STATES 3868683442\n",
      "DETAILS PROFILE\n",
      "1515 Pacific Ave Dedicated Data Engineer with 5+ years’ experience dealing with large datasets. Eager\n",
      "Los Angeles, CA 90291 to build robust databases that lay the groundwork for game-changing insights at\n",
      "United States LionHeart Algorithm LLC. Implemented natural language processing tools to ensure\n",
      "3868683442 machine-readable databases were ready for the team of data scientists.\n",
      "email@email.com\n",
      "EMPLOYMENT HISTORY\n",
      "Place of birth\n",
      "San Antonio\n",
      "Data Engineer at FNB, Nong Phai\n",
      "January 2018 — December 2020\n",
      "Driving license\n",
      "Full Responsible for scaling machine learning models and making these models fit within\n",
      "banking environments with the sci-kit-learn, Tensorflow, and Keras, also adapting these\n",
      "SKILLS models/architectures created by data scientists to fit in with the UX and the UI of the brand.\n",
      "SQL • Translated business propositions into quantitative queries and organized the\n",
      "necessary data.\n",
      "Java • Developed scalable databases capable of ETL processes using SQL and Spark.\n",
      "• Estimated the workflow and increase the efficiency of data pipelines that process\n",
      "Apache Spark\n",
      "over 60 TB of data daily.\n",
      "Hadoop • Utilized MongoDB to create NoSQL databases that collect data from a variety of\n",
      "sources.\n",
      "Python • Involved in creating tables, join conditions, partitioning tables, correlated\n",
      "subqueries, views, sequences, nested queries, and synonyms for business\n",
      "Coding\n",
      "application development.\n",
      "LANGUAGES\n",
      "Data Engineer at ABSA, Washington\n",
      "English January 2015 — December 2017\n",
      "Responsible for developing database triggers, packages, functions, and stored procedures\n",
      "German\n",
      "using PL/SQL and maintain the scripts for various data feeds across multiple regional and\n",
      "international offices of the company\n",
      "HOBBIES\n",
      "• Co-develop a SQL server database system to maximize performance benefits for\n",
      "Cycling, Songwriting, Running clientele.\n",
      "• Assisted senior-level Data Scientists in the design of ETL processes, including SSIS\n",
      "packages.\n",
      "• Developed coherent Logical Data Models that helped guide important client\n",
      "business decisions.\n",
      "• Collaborate and coordinate with development teams to deploy data quality\n",
      "solutions and create and maintain standard operating procedure documentation.\n",
      "EDUCATION\n",
      "BS, Computer Science, Texas University, Austin\n",
      "January 2021 — January 2021\n",
      "• Dual concentration Machine Learning,\n",
      "• Received a Business Foundations Certificate\n",
      "• Part of the winning team for the 2016 Longhorn.COURSES\n",
      "CCA Cloudera Certified Associate, Cloudera, Online.\n",
      "January 2021 — January 2021\n",
      "ACHIEVEMENTS\n",
      "• Developed a data pipeline with Delta Lake that led to process optimization and a\n",
      "corresponding revenue increase of 21%.\n",
      "• Successfully figured out ETL issue while following PL/SQL best practices that\n",
      "resulted in an insight that increased the client’s customer base by 37%.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Thaddeus Drake.pdf\n",
      "\n",
      "THADDEUS WORK EXPERIENCE\n",
      "DRAKE Junior Data Engineer\n",
      "Illumina\n",
      "Junior Data Engineer\n",
      "2023 - current San Diego, CA\n",
      "Developed and optimized MySQL queries to manage a\n",
      "t.drake@email.com database of 500,046 genomic records, improving data\n",
      "retrieval speed by 32%\n",
      "(123) 456-7890\n",
      "Automated data ingestion workflows with Apache NiFi,\n",
      "San Diego, CA\n",
      "processing over 2 TB of genomic data daily across multiple\n",
      "LinkedIn\n",
      "sources\n",
      "Deployed and maintained data pipelines on AWS, reducing\n",
      "the ETL processing time by 44% for clinical research datasets\n",
      "EDUCATION Implemented and managed Apache Airflow for scheduling\n",
      "over 53 daily data workflows, ensuring reliable data\n",
      "Bachelor of Science\n",
      "integration and processing\n",
      "Computer Science\n",
      "University of California Data Engineer Intern\n",
      "2019 - 2023\n",
      "Teradata\n",
      "San Diego, CA\n",
      "2022 - 2023 San Diego, CA\n",
      "Migrated a legacy data warehouse to Snowflake, optimizing\n",
      "storage and querying for over 103 million rows of customer\n",
      "SKILLS\n",
      "data\n",
      "MySQL Created Python scripts for automating data extraction and\n",
      "transformation processes, saving 17 hours of manual work\n",
      "Apache NiFi\n",
      "each week\n",
      "AWS\n",
      "Integrated and optimized ETL workflows using Apache\n",
      "Apache Airflow\n",
      "Hadoop, processing 3 TB of raw data for reporting purposes\n",
      "Apache Hadoop\n",
      "Used Git to version control and collaborate on data pipeline\n",
      "Snowflake development, ensuring code integrity and efficient team\n",
      "Python workflows\n",
      "Git\n",
      "dbt Data Entry Clerk\n",
      "Tableau\n",
      "Epsilon\n",
      "2021 - 2022 San Diego, CA\n",
      "Managed and validated large datasets in MySQL, improving\n",
      "the accuracy of customer information across a database of 1\n",
      "million records\n",
      "Built 12 Tableau dashboards for visualizing customer data\n",
      "trends, used by marketing teams for targeted campaigns\n",
      "Processed and veri\u0000ed over 5,021 data entries per week,\n",
      "ensuring high-quality input for marketing campaigns\n",
      "Assisted with daily ETL processes using Apache NiFi and dbt\n",
      "to ingest and transform 12GB of client data daily for\n",
      "reporting\n",
      "-------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "roles = [\"data engineer\",\"full stack developer\",\"machine learning engineer\",\"data scientist\"]\n",
    "seniority_dict = {\n",
    "    \"senior\" : \"more than 4 years of experience in total\",\n",
    "    \"mid-senior\" : \"between 2 and 4 years of experience in total\",\n",
    "    \"junior\" : \"between 0 and 2 years of experience in total\"\n",
    "}\n",
    "\n",
    "# Especifica el directorio que quieres iterar\n",
    "directorio = './resources/cvs_landing'\n",
    "\n",
    "descriptions_dict = {}\n",
    "# Itera sobre los archivos en el directorio\n",
    "for nombre_archivo in os.listdir(directorio):\n",
    "    ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "    if os.path.isfile(ruta_archivo):\n",
    "        cv_text = extract_text_from_cv(ruta_archivo)\n",
    "        if cv_text is not None:\n",
    "            \n",
    "            words = cv_text.split()    \n",
    "            num_of_words = len(words)\n",
    "\n",
    "            if num_of_words > 5:    \n",
    "                descriptions_dict[nombre_archivo] = cv_text\n",
    "\n",
    "for filename,desc in descriptions_dict.items():\n",
    "    \n",
    "        descriptions_dict[filename] = desc\n",
    "        print(filename+\"\\n\\n\"+desc+\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4060d846-3f9d-4dca-af0f-7318ec4f0188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of Candidates**\n",
       "\n",
       "### Senior-Level Candidates\n",
       "\n",
       "1. **Elara Quinn**\n",
       "\t* Experience: 4+ years, Data Engineer Intern at HP Inc.\n",
       "\t* Technologies: Apache Hadoop, MySQL, Tableau, AWS\n",
       "\t* Relevant Projects: Built pipelines with Apache Hadoop, processed 4 terabytes of raw data for analytics projects\n",
       "2. **Ianthe Marlow**\n",
       "\t* Experience: 5+ years, Data Engineer at Facebook\n",
       "\t* Technologies: Airflow, Presto, Apache Hive, Amazon S3\n",
       "\t* Relevant Projects: Automated ETL processes across billions of rows of data, reduced manual intervention by 6 hours per month\n",
       "3. **John Smith**\n",
       "\t* Experience: 5+ years, Data Engineer at LionHeart Algorithm LLC\n",
       "\t* Technologies: SQL, Java, Hadoop, MongoDB, Python\n",
       "\t* Relevant Projects: Developed scalable databases capable of ETL processes using SQL and Spark\n",
       "\n",
       "### Mid-Level Candidates\n",
       "\n",
       "1. **Thaddeus Drake**\n",
       "\t* Experience: 3+ years, Junior Data Engineer at Illumina\n",
       "\t* Technologies: MySQL, Apache NiFi, AWS\n",
       "\t* Relevant Projects: Automated data ingestion workflows with Apache NiFi, processed over 2 TB of genomic data daily\n",
       "2. **Brandon Connor**\n",
       "\t* Experience: 1+ year, Data Engineering Intern at Balyasny\n",
       "\t* Technologies: Python, SQL, Spark, Tableau\n",
       "\t* Relevant Projects: Built a cloud-first data ingestion pipeline that improved processing speed by 74%\n",
       "\n",
       "### Emerging Candidates\n",
       "\n",
       "1. **Alan Susa**\n",
       "\t* Experience: 2+ years, Data Engineer Intern at Consumer Reports\n",
       "\t* Technologies: PySpark, Kafka, Airflow, AWS\n",
       "\t* Relevant Projects: Designed and implemented a real-time data pipeline to process semi-structured data from 30+ sources\n",
       "2. **Ianthe Marlow**\n",
       "\t* (Duplicate entry, already listed as Senior-Level Candidate)\n",
       "3. **John Smith**\n",
       "\t* (Duplicate entry, already listed as Senior-Level Candidate)\n",
       "\n",
       "Note: The seniority level and technologies required for the position are emphasized in the selection process. The candidates have varying levels of experience and expertise, but only a few meet the senior-level requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "response = ollama.chat(\n",
    "model=\"llama3.2\",\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"reorder me the following candidates dictionary {descriptions_dict} from the most interesting candidate to the less for the job description: {job_desc_text},\n",
    "        pay attention to the seniority level rquired and the technologies that the candidates manage. the answer has to be ready to be printed in markdown and a summarized description of all the candidates.\"\"\",\n",
    "    }\n",
    "],\n",
    ")\n",
    "llm_response = response[\"message\"][\"content\"]\n",
    "display(Markdown(llm_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb2b37-dccd-4630-85b3-889c4cb8f013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
