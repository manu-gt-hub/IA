{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552f7439-9171-4eb7-9168-1278b61f38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCLAIMER:\n",
    "# \n",
    "# all CVs used here were found in: https://www.beamjobs.com/resumes/data-engineer-resume-examples\n",
    "\n",
    "# CONCLUSIONS\n",
    "#\n",
    "# the key point using LLAMA3 is to retrieve the seniority from the CV (overall and specific)\n",
    "# we could use another tool to extract it and send it to the LLM already gathered to improve its performance\n",
    "# alternative: use the LLM to get every experience the candidate has and calculate and store the experience using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78532789-97ed-48db-91ce-2cd4ad13b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import pprint\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "seniority = \"senior\"\n",
    "\n",
    "job_desc_text = f\"\"\"\n",
    "Position Description:\n",
    "\n",
    "We are seeking a {seniority} Data Engineer to design, build, and maintain scalable data pipelines. The ideal candidate will have expertise in SQL, ETL processes, and cloud technologies, collaborating closely with Data Scientists to ensure data integration and quality.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Manage cloud data storage systems.\n",
    "Collaborate with data scientists to meet data requirements.\n",
    "Ensure data quality and security.\n",
    "Automate data processes.\n",
    "Monitor and troubleshoot data systems.\n",
    "Optimize Big Data solutions.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Education: Degree in Computer Science or related field.\n",
    "Proficiency in programming languages such as Python, Go, or Rust.\n",
    "Strong SQL skills.\n",
    "Experience with Hadoop and Kafka.\n",
    "Familiarity with cloud platforms (IBM Cloud, Oracle Cloud).\n",
    "Knowledge of data orchestration tools like Prefect or Luigi.\n",
    "Experience in CI/CD tools (GitLab CI, CircleCI).\n",
    "\n",
    "Desirable:\n",
    "\n",
    "Experience with Snowflake.\n",
    "Knowledge of visualization tools (Tableau, PowerBI).\n",
    "Familiarity with Docker or Kubernetes.\n",
    "Understanding of agile methodologies.\n",
    "Cloud or big data certifications.\n",
    "Multicultural experience.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f23468-20ea-407e-ac85-cc8e2cf6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Función para leer el contenido de un archivo .docx\n",
    "def read_docx(cv_path):\n",
    "    try:\n",
    "        # Cargar el documento\n",
    "        doc = Document(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = []\n",
    "        for parrafo in doc.paragraphs:\n",
    "            contenido.append(parrafo.text)\n",
    "        \n",
    "        return '\\n'.join(contenido)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_doc(cv_path):\n",
    "    try:\n",
    "        # Inicializar la aplicación de Word\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        \n",
    "        # Abrir el documento\n",
    "        doc = word.Documents.Open(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = doc.Content.Text\n",
    "        \n",
    "        # Cerrar el documento y la aplicación de Word\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        \n",
    "        return contenido\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_cv(cv_path):\n",
    "    cv_text = read_pdf(cv_path)\n",
    "    if cv_text is None:\n",
    "        cv_text = read_docx(cv_path)\n",
    "        if cv_text is None:\n",
    "            cv_text =read_doc(cv_path)\n",
    "\n",
    "    return cv_text\n",
    "\n",
    "\n",
    "def get_job_description(url):\n",
    "    class_name = 'wiki-content'\n",
    "    # Hacer la solicitud HTTP\n",
    "    response = requests.get(url, verify = False)\n",
    "    \n",
    "    # Verificar que la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Analizar el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encontrar todos los elementos con la clase especificada\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "        \n",
    "        # Extraer el texto de esos elementos\n",
    "        text = '\\n'.join([element.get_text(separator='\\n').strip() for element in elements])\n",
    "        return text\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch the page. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3430549e-7dd9-4de9-8d28-ea5163613c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "roles = [\"data engineer\",\"full stack developer\",\"machine learning engineer\",\"data scientist\"]\n",
    "seniority_dict = {\n",
    "    \"senior\" : \"more than 4 years of experience in total\",\n",
    "    \"mid-senior\" : \"between 2 and 4 years of experience in total\",\n",
    "    \"junior\" : \"between 0 and 2 years of experience in total\"\n",
    "}\n",
    "\n",
    "directorio = './resources/cvs_landing'\n",
    "\n",
    "descriptions_dict = {}\n",
    "\n",
    "for nombre_archivo in os.listdir(directorio):\n",
    "    ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "    if os.path.isfile(ruta_archivo):\n",
    "        cv_text = extract_text_from_cv(ruta_archivo)\n",
    "        if cv_text is not None:\n",
    "            \n",
    "            words = cv_text.split()    \n",
    "            num_of_words = len(words)\n",
    "\n",
    "            if num_of_words > 5:    \n",
    "                descriptions_dict[nombre_archivo] = cv_text\n",
    "\n",
    "for filename,desc in descriptions_dict.items():\n",
    "    \n",
    "        descriptions_dict[filename] = desc\n",
    "        #print(filename+\"\\n\\n\"+desc+\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee16bc2-85e6-4cb7-8c6c-d53a48ed8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_prompt():\n",
    "    prompt = \"\"\n",
    "    \n",
    "    prompt += f\"\"\"answer me with a list of candidates based on this dictionary: \\n\\n\"\"\"\n",
    "    \n",
    "    for fn,desc in descriptions_dict.items():\n",
    "        prompt += desc + \"\\n\\n\"\n",
    "    \n",
    "    prompt += f\"\"\"from the first interesting candidate we should interview to the less for the job description: {job_desc_text},\n",
    "            pay attention to the candidate's seniority level and the rquired for the job, also the technologies that the candidates manage. the answer has to be ready to be printed in markdown and a summarized description of all the candidates.\"\"\"\n",
    "    return prompt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4060d846-3f9d-4dca-af0f-7318ec4f0188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Candidate Summary**\n",
       "\n",
       "We've shortlisted 5 candidates for the Senior Data Engineer position based on their experience, skills, and education.\n",
       "\n",
       "### 1. Marlowe (Facebook)\n",
       "* Seniority Level: Experienced\n",
       "* Technologies Managed: Airflow, Presto, Apache Hive, AWS, MySQL\n",
       "* Education: Bachelor of Science in Computer Science from Carnegie Mellon University\n",
       "\n",
       "### 2. Elara Quinn (HP Inc.)\n",
       "* Seniority Level: Entry-Level\n",
       "* Technologies Managed: Apache Hadoop, Tableau, MySQL, Apache NiFi\n",
       "* Education: Bachelor of Science in Computer Science from the University of Texas\n",
       "\n",
       "### 3. John Smith (LionHeart Algorithm LLC)\n",
       "* Seniority Level: Experienced\n",
       "* Technologies Managed: Natural Language Processing, SQL, Java, Apache Spark, MongoDB\n",
       "* Education: Bachelor of Science in Computer Science from Texas University, Austin\n",
       "\n",
       "### 4. Thaddeus (Illumina)\n",
       "* Seniority Level: Entry-Level\n",
       "* Technologies Managed: MySQL, Apache NiFi, AWS, Snowflake, Hadoop\n",
       "* Education: Bachelor of Science in Computer Science from the University of California\n",
       "\n",
       "### 5. Brandon Connor (Simplex)\n",
       "* Seniority Level: Junior\n",
       "* Technologies Managed: Python, SQL, Kafka, Spark\n",
       "* Education: B.S. in Computer Science from the University of Texas"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLAMA3 TEST\n",
    "\n",
    "response = ollama.chat(\n",
    "model=\"llama3.2\",\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": generate_prompt()\n",
    "    }\n",
    "],\n",
    ")\n",
    "llm_response = response[\"message\"][\"content\"]\n",
    "display(Markdown(llm_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "009e886d-f30d-405d-87f4-86ef8672f7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Candidates Ranked for Senior Data Engineer Position\n",
       "\n",
       "## 1. **ALAN SUSA**\n",
       "- **Experience:** 12+ years in data engineering with robust experience in scaling data pipelines.\n",
       "- **Technologies:** SQL (Postgres, Redshift, MySQL), Python, Kafka, PySpark, AWS, Airflow.\n",
       "- **Summary:** Led significant projects that resulted in substantial cost savings and performance increases. Has a strong history in designing and implementing scalable data pipelines and automating ETL processes. A proven track record of optimizing system performance.\n",
       "\n",
       "## 2. **JOHN SMITH**\n",
       "- **Experience:** 8+ years in data engineering, with solid experience in ETL processes and machine learning model adaptation.\n",
       "- **Technologies:** SQL, Python, Apache Spark, NoSQL (MongoDB).\n",
       "- **Summary:** Extensive experience in developing scalable databases and implementing data quality solutions. Has a hands-on approach with cloud services and has successfully optimized revenue through efficient data processes.\n",
       "\n",
       "## 3. **IANTHE MARLOWE**\n",
       "- **Experience:** 4+ years in data engineering, including advanced skills in data pipeline management and cloud integration.\n",
       "- **Technologies:** SQL, Apache Airflow, Apache Hive, Amazon S3, Apache Kafka, Presto.\n",
       "- **Summary:** Focused on automating workflows and optimizing data processes with a strong understanding of cloud services. Demonstrated ability to improve query performance and data storage management effectively.\n",
       "\n",
       "## 4. **THADDEUS DRAKE**\n",
       "- **Experience:** 4 years of combined experience as a Junior Data Engineer and intern roles.\n",
       "- **Technologies:** MySQL, Apache NiFi, AWS, Apache Airflow.\n",
       "- **Summary:** Proven track record of managing large datasets and automating data workflows. Successfully reduced processing times and increased data retrieval speed, aligning with the job's requirements.\n",
       "\n",
       "## 5. **BRANDON CONNOR**\n",
       "- **Experience:** 3+ years as an intern in data engineering, focusing mostly on developing data ingestion and pipeline processes.\n",
       "- **Technologies:** Python, SQL, Spark, AWS, Google Analytics API.\n",
       "- **Summary:** Practical experience in collaborative projects with notable success metrics. While still in a junior role, his project accomplishments indicate potential for growth in a senior position.\n",
       "\n",
       "## 6. **ELARA QUINN**\n",
       "- **Experience:** 2+ years as an intern with project work in data engineering.\n",
       "- **Technologies:** MySQL, Apache NiFi, Amazon Redshift, Tableau.\n",
       "- **Summary:** Strong foundational knowledge and practical skills through internships. Nevertheless, lacks extensive experience compared to seniority requirements.\n",
       "\n",
       "---\n",
       "\n",
       "Candidates were evaluated based on their experience, technologies managed, and alignment with job responsibilities and requirements. Alan Susa stands out with the most experience and relevant skills, followed closely by John Smith."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPT 4-o-mini TEST\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "openai = OpenAI(http_client=httpx.Client(verify=False))\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a candidates classifier, I'm going to provide you some candidates description and you are going to order them from most interesting to interview to the low.\"},\n",
    "        {\"role\": \"user\", \"content\": generate_prompt()}\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpt_answer = response.choices[0].message.content\n",
    "display(Markdown(gpt_answer))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
