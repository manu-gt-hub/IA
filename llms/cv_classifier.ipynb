{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552f7439-9171-4eb7-9168-1278b61f38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISCLAIMER:\n",
    "#\n",
    "# all CVs used here were found in: https://www.beamjobs.com/resumes/data-engineer-resume-examples\n",
    "\n",
    "# CONCLUSIONS\n",
    "#\n",
    "# the key point using LLAMA3 is to retrieve the seniority from the CV (overall and specific)\n",
    "# we could use another tool to extract it and send it to the LLM already gathered to improve its performance\n",
    "# alternative: use the LLM to get every experience the candidate has and calculate and store the experience using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78532789-97ed-48db-91ce-2cd4ad13b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ollama\n",
    "from IPython.display import Markdown, display\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import pprint\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "seniority = \"senior\"\n",
    "\n",
    "job_desc_text = f\"\"\"\n",
    "Position Description:\n",
    "\n",
    "We are seeking a {seniority} Data Engineer to design, build, and maintain scalable data pipelines. The ideal candidate will have expertise in SQL, ETL processes, and cloud technologies, collaborating closely with Data Scientists to ensure data integration and quality.\n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "Manage cloud data storage systems.\n",
    "Collaborate with data scientists to meet data requirements.\n",
    "Ensure data quality and security.\n",
    "Automate data processes.\n",
    "Monitor and troubleshoot data systems.\n",
    "Optimize Big Data solutions.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Education: Degree in Computer Science or related field.\n",
    "Proficiency in programming languages such as Python, Go, or Rust.\n",
    "Strong SQL skills.\n",
    "Experience with Hadoop and Kafka.\n",
    "Familiarity with cloud platforms (IBM Cloud, Oracle Cloud).\n",
    "Knowledge of data orchestration tools like Prefect or Luigi.\n",
    "Experience in CI/CD tools (GitLab CI, CircleCI).\n",
    "\n",
    "Desirable:\n",
    "\n",
    "Experience with Snowflake.\n",
    "Knowledge of visualization tools (Tableau, PowerBI).\n",
    "Familiarity with Docker or Kubernetes.\n",
    "Understanding of agile methodologies.\n",
    "Cloud or big data certifications.\n",
    "Multicultural experience.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f23468-20ea-407e-ac85-cc8e2cf6f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Función para leer el contenido de un archivo .docx\n",
    "def read_docx(cv_path):\n",
    "    try:\n",
    "        # Cargar el documento\n",
    "        doc = Document(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = []\n",
    "        for parrafo in doc.paragraphs:\n",
    "            contenido.append(parrafo.text)\n",
    "        \n",
    "        return '\\n'.join(contenido)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_doc(cv_path):\n",
    "    try:\n",
    "        # Inicializar la aplicación de Word\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        word.Visible = False\n",
    "        \n",
    "        # Abrir el documento\n",
    "        doc = word.Documents.Open(cv_path)\n",
    "        \n",
    "        # Leer el contenido del documento\n",
    "        contenido = doc.Content.Text\n",
    "        \n",
    "        # Cerrar el documento y la aplicación de Word\n",
    "        doc.Close(False)\n",
    "        word.Quit()\n",
    "        \n",
    "        return contenido\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_text_from_cv(cv_path):\n",
    "    cv_text = read_pdf(cv_path)\n",
    "    if cv_text is None:\n",
    "        cv_text = read_docx(cv_path)\n",
    "        if cv_text is None:\n",
    "            cv_text =read_doc(cv_path)\n",
    "\n",
    "    return cv_text\n",
    "\n",
    "\n",
    "def get_job_description(url):\n",
    "    class_name = 'wiki-content'\n",
    "    # Hacer la solicitud HTTP\n",
    "    response = requests.get(url, verify = False)\n",
    "    \n",
    "    # Verificar que la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Analizar el contenido HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Encontrar todos los elementos con la clase especificada\n",
    "        elements = soup.find_all(class_=class_name)\n",
    "        \n",
    "        # Extraer el texto de esos elementos\n",
    "        text = '\\n'.join([element.get_text(separator='\\n').strip() for element in elements])\n",
    "        return text\n",
    "    else:\n",
    "        return f\"Error: Unable to fetch the page. Status code: {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3430549e-7dd9-4de9-8d28-ea5163613c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    }
   ],
   "source": [
    "roles = [\"data engineer\",\"full stack developer\",\"machine learning engineer\",\"data scientist\"]\n",
    "seniority_dict = {\n",
    "    \"senior\" : \"more than 4 years of experience in total\",\n",
    "    \"mid-senior\" : \"between 2 and 4 years of experience in total\",\n",
    "    \"junior\" : \"between 0 and 2 years of experience in total\"\n",
    "}\n",
    "\n",
    "directorio = './resources/cvs_landing'\n",
    "\n",
    "descriptions_dict = {}\n",
    "\n",
    "for nombre_archivo in os.listdir(directorio):\n",
    "    ruta_archivo = os.path.join(directorio, nombre_archivo)\n",
    "    if os.path.isfile(ruta_archivo):\n",
    "        cv_text = extract_text_from_cv(ruta_archivo)\n",
    "        if cv_text is not None:\n",
    "            \n",
    "            words = cv_text.split()    \n",
    "            num_of_words = len(words)\n",
    "\n",
    "            if num_of_words > 5:    \n",
    "                descriptions_dict[nombre_archivo] = cv_text\n",
    "\n",
    "for filename,desc in descriptions_dict.items():\n",
    "    \n",
    "        descriptions_dict[filename] = desc\n",
    "        #print(filename+\"\\n\\n\"+desc+\"\\n-------------------------------------------------------------------------------\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee16bc2-85e6-4cb7-8c6c-d53a48ed8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_prompt():\n",
    "    prompt = \"\"\n",
    "    \n",
    "    prompt += f\"\"\"answer me with a list of candidates based on this dictionary: \\n\\n\"\"\"\n",
    "    \n",
    "    for fn,desc in descriptions_dict.items():\n",
    "        prompt += desc + \"\\n\\n\"\n",
    "    \n",
    "    prompt += f\"\"\"from the first interesting candidate we should interview to the less for the job description: {job_desc_text},\n",
    "            pay attention to the candidate's seniority level and the rquired for the job, also the technologies that the candidates manage. the answer has to be ready to be printed in markdown and a summarized description of all the candidates.\"\"\"\n",
    "    return prompt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4060d846-3f9d-4dca-af0f-7318ec4f0188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Candidate Comparison Summary\n",
       "Based on the provided dictionaries, here is a summary comparison of the candidates' suitability for the senior Data Engineer position.\n",
       "\n",
       "## Seniority Level:\n",
       "1. Alan Susa - Mid-level (5+ years of experience)\n",
       "2. Brandon Connor - Junior (Internship experience)\n",
       "3. Elara Quinn - Mid-level (2+ years of experience)\n",
       "4. Marlowe - Mid-level (2+ years of experience)\n",
       "5. John Smith - Senior (5+ years of experience)\n",
       "6. Thaddeus Drake - Mid-level (1+ year of experience)\n",
       "\n",
       "## Required Technologies:\n",
       "1. Alan Susa: Amazon Athena, S3, Kafka, PySpark\n",
       "2. Brandon Connor: Python, SQL, Spark\n",
       "3. Elara Quinn: Apache Hadoop, MySQL, Tableau, AWS\n",
       "4. Marlowe: Airflow, Presto, Hive\n",
       "5. John Smith: Machine learning tools (TensorFlow, Keras), SQL, Java\n",
       "6. Thaddeus Drake: MySQL, Apache NiFi, Snowflake, AWS\n",
       "\n",
       "## Ranking from Most Suitable to Least:\n",
       "1. John Smith - Senior level with 5+ years of experience and expertise in machine learning tools.\n",
       "2. Alan Susa - Mid-level with 5+ years of experience and expertise in cloud technologies (Athena, S3, Kafka).\n",
       "3. Marlowe - Mid-level with 2+ years of experience and expertise in data orchestration tools (Airflow).\n",
       "4. Elara Quinn - Mid-level with 2+ years of experience and expertise in Big Data solutions (Hadoop, MySQL, Tableau).\n",
       "5. Thaddeus Drake - Mid-level with 1+ year of experience and expertise in cloud technologies (AWS, Snowflake).\n",
       "6. Brandon Connor - Junior with internship experience.\n",
       "\n",
       "Note: The ranking is based on the candidate's seniority level, required technologies, and overall fit for the position."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LLAMA3 TEST\n",
    "\n",
    "response = ollama.chat(\n",
    "model=\"llama3.2\",\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": generate_prompt()\n",
    "    }\n",
    "],\n",
    ")\n",
    "llm_response = response[\"message\"][\"content\"]\n",
    "display(Markdown(llm_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009e886d-f30d-405d-87f4-86ef8672f7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "# Candidate Ranking for Senior Data Engineer Position\n",
      "\n",
      "1. **ALAN SUSA**\n",
      "   - **Experience**: Senior Data Engineer with significant expertise in SQL, ETL processes, and cloud technologies (AWS). Proven track record in data pipeline design and implementation.\n",
      "   - **Relevant Skills**: Python, SQL (Postgres, Redshift, MySQL), Spark, Kafka, AWS.\n",
      "   - **Education**: B.A. in Computer Science.\n",
      "   - **Highlights**: Led migration projects that resulted in substantial cost savings and performance improvements. Managed high-uptime data pipelines and automated ETL processes.\n",
      "\n",
      "2. **JOHN SMITH**\n",
      "   - **Experience**: Dedicated Data Engineer with over 5 years of experience in dealing with large datasets, including banking environments and adaptable ML models.\n",
      "   - **Relevant Skills**: SQL, Apache Spark, MongoDB, Python, Hadoop.\n",
      "   - **Education**: B.S. in Computer Science.\n",
      "   - **Highlights**: Developed scalable databases and enhanced efficiency of data pipelines. Experienced in ETL processes with proven results.\n",
      "\n",
      "3. **IANTHE MARLOWE**\n",
      "   - **Experience**: Data Engineer at Facebook with experience in automating data workflows and optimizing data performance.\n",
      "   - **Relevant Skills**: Apache Airflow, Presto, Amazon S3, Apache Kafka, SQL.\n",
      "   - **Education**: B.S. in Computer Science.\n",
      "   - **Highlights**: Achieved significant performance improvements through integration of new technologies. Experience with real-time event streaming and building data warehousing solutions.\n",
      "\n",
      "4. **BRANDON CONNOR**\n",
      "   - **Experience**: Data Engineering Intern with significant project experience and successful collaborations in a cloud environment.\n",
      "   - **Relevant Skills**: Python, SQL, Spark, ETLs.\n",
      "   - **Education**: B.S. in Computer Science.\n",
      "   - **Highlights**: Optimized data processing and created effective BI tools reporting. Demonstrated strong ability to translate feedback into actionable reports.\n",
      "\n",
      "5. **THADDEUS DRAKE**\n",
      "   - **Experience**: Junior Data Engineer with solid experience managing and optimizing data workflows for clinical research.\n",
      "   - **Relevant Skills**: MySQL, Apache NiFi, AWS, Apache Airflow.\n",
      "   - **Education**: B.S. in Computer Science.\n",
      "   - **Highlights**: Developed and maintained data pipelines, automated ETL processes, and improved data retrieval speed. Lacks seniority but relevant skills.\n",
      "\n",
      "6. **ELARA QUINN**\n",
      "   - **Experience**: Data Engineer Intern with hands-on experience in pipeline development and analytics.\n",
      "   - **Relevant Skills**: MySQL, Apache NiFi, Amazon Redshift, Tableau.\n",
      "   - **Education**: B.S. in Computer Science (ongoing).\n",
      "   - **Highlights**: Gained practical experience but lacks the depth of experience required for a senior role. Focused on real-world applications of data engineering tools.\n",
      "\n",
      "```\n",
      "This ranking of candidates takes into account their experience, relevant skills, education, and accomplishments in relation to the responsibilities and requirements of the Senior Data Engineer position described.\n"
     ]
    }
   ],
   "source": [
    "# GPT 4-o-mini TEST\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "openai = OpenAI(http_client=httpx.Client(verify=False))\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a candidates classifier, I'm going to provide you some candidates description and you are going to order them from most interesting to interview to the low\"},\n",
    "        {\"role\": \"user\", \"content\": generate_prompt()}\n",
    "    ]\n",
    ")\n",
    "\n",
    "gpt_answer = response.choices[0].message.content\n",
    "print(gpt_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
